{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design a Perturbed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset and Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    \"\"\"Time Series Dataset\n",
    "    A sample consists of a (random) time window + consecutive time horizon.\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        file_path (str): path to csv file containing the data\n",
    "        input_len (int): length of input sequence\n",
    "        target_len (int): length of target sequence\n",
    "        stride (int): stride between samples. Only used if n_samples is None and samples are thus drawn sequentially.\n",
    "        n_samples (int): number of samples to draw. If None, all possible samples are drawn sequentially. Else, samples are drawn randomly.\n",
    "        mean_vals (np.array): mean values for each feature. If given, data is rescaled to have zero mean.\n",
    "        sd_vals (np.array): standard deviation values for each feature. If given, data is rescaled to have unit variance.\n",
    "        continuous_features (list): list of continuous features\n",
    "        discrete_features (list): list of discrete features\n",
    "        seed (int): seed for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 df=None,\n",
    "                 file_path=None,\n",
    "                 input_len=90,\n",
    "                 target_len=30,\n",
    "                 stride=1,\n",
    "                 n_samples=None,\n",
    "                 mean_vals=None,\n",
    "                 sd_vals=None,\n",
    "                 continuous_features=None,\n",
    "                 discrete_features=None,\n",
    "                 seed=42\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "        elif file_path is not None:\n",
    "            if file_path.endswith(\".parquet\"):\n",
    "                self.df = pd.read_parquet(file_path)\n",
    "            elif file_path.endswith(\".csv\"):\n",
    "                self.df = pd.read_csv(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"File format not supported.\")\n",
    "            try:  # Try to convert the first column to datetime. If not possible, ignore it.\n",
    "                self.df.set_index(pd.to_datetime(self.df.iloc[:,0], format=\"%Y-%m-%d %H:%M:%S\"), inplace=True)\n",
    "                self.df.drop(self.df.columns[0], axis=1, inplace=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\"Either df or file_path must be given.\")\n",
    "\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        self.stride = stride  # only used if n_samples is None and samples are drawn sequentially\n",
    "\n",
    "        self.random_sampling = n_samples is not None\n",
    "        self.n_samples = self.__len__() if n_samples is None else n_samples\n",
    "        assert self.n_samples <= self.__len__(), \"n_samples must be smaller than the number of possible samples.\"\n",
    "        self.n_features = self.df.shape[1]\n",
    "        self.feature_names = self.df.columns\n",
    "        self.continuous_features, self.discrete_features = self.split_hybrid_data(continuous_features, discrete_features)\n",
    "\n",
    "        # Rescale data if min and max values are given\n",
    "        self.mean_vals = mean_vals\n",
    "        self.sd_vals = sd_vals\n",
    "        if mean_vals is not None and sd_vals is not None:\n",
    "            self.scale_data()\n",
    "\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)  # Using a local random number generator\n",
    "        else:\n",
    "            self.rng = np.random.default_rng()  # Default random generator without a fixed seed\n",
    "\n",
    "        self.sample_idxs = self._create_sample_indices()\n",
    "\n",
    "    def split_hybrid_data(self, continuous_features=None, discrete_features=None):\n",
    "        \"\"\"Split the time series data features into continuous and discrete features.\"\"\"\n",
    "        continuous_threshold = 32\n",
    "        continuous_features = [feature for feature in self.df.columns if self.df[feature].nunique() > continuous_threshold] if continuous_features is None else continuous_features\n",
    "        discrete_features = [feature for feature in self.df.columns if self.df[feature].nunique() <= continuous_threshold] if discrete_features is None else discrete_features\n",
    "        assert len(continuous_features) + len(discrete_features) == self.n_features, \"All features must be assigned to either continuous or discrete features.\"\n",
    "        return continuous_features, discrete_features\n",
    "\n",
    "    def set_scaler_params(self, mean_vals=None, sd_vals=None):\n",
    "        \"\"\"Set the parameters for scaling the data.\n",
    "        Args:\n",
    "            mean_vals (np.array): mean values for each feature. If given, data is rescaled to have zero mean.\n",
    "            sd_vals (np.array): standard deviation values for each feature. If given, data is rescaled to have unit variance.\n",
    "        \"\"\"\n",
    "        self.mean_vals = mean_vals if mean_vals is not None else self.df.mean()\n",
    "        self.sd_vals = sd_vals if sd_vals is not None else self.df.std()\n",
    "\n",
    "    def scale_data(self):\n",
    "        \"\"\"Scale data between min and max values.\"\"\"\n",
    "        assert self.mean_vals is not None and self.sd_vals is not None, \"Mean and standard deviation values must be set first.\"\n",
    "        # Avoid division by zero by replacing sd value of 0 with 1 (for constant features)\n",
    "        self.sd_vals.replace(0, 1.0, inplace=True)\n",
    "        # Standardize the data\n",
    "        self.df = (self.df - self.mean_vals) / self.sd_vals\n",
    "\n",
    "    def inverse_scale_data(self, scaled_data):\n",
    "        df_ = pd.DataFrame(scaled_data, columns=self.df.columns)\n",
    "        return (df_ * self.sd_vals) + self.mean_vals\n",
    "\n",
    "    def _create_sample_indices(self):\n",
    "        \"\"\"Create an array of indices for sampling\"\"\"\n",
    "        if self.random_sampling:\n",
    "            sample_idxs = self.rng.integers(low=0, high=self.df.shape[0] - 2 * self.input_len - self.target_len, size=self.n_samples)  # -2*input_len because some perturbations might require more than input_len time steps\n",
    "        else:\n",
    "            max_n_samples = int((self.df.shape[0] - 2 * self.input_len - self.target_len) / self.stride) + 1  # -2*input_len because some perturbations might require more than input_len time steps\n",
    "            sample_idxs = np.arange(max_n_samples) * self.stride\n",
    "        return sample_idxs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples\"\"\"\n",
    "        if self.random_sampling:\n",
    "            return self.n_samples\n",
    "        else:\n",
    "            return int((self.df.shape[0] - self.input_len - self.target_len) / self.stride) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get one sample.\n",
    "        A sample consists of a time window of length input_len and a consecutive time horizon of length target_len.\n",
    "        Returns:\n",
    "            x (np.array): input sequence\n",
    "            y (np.array): target sequence\n",
    "        \"\"\"\n",
    "        start_idx = self.sample_idxs[index]\n",
    "        end_idx = start_idx + self.input_len + self.target_len\n",
    "        df_ = self.df.iloc[start_idx:end_idx]\n",
    "        x = df_.iloc[:self.input_len].to_numpy().astype(np.float32)\n",
    "        y = df_.iloc[self.input_len:].to_numpy().astype(np.float32)\n",
    "        del df_\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_tank_filepath = '../data/processed/three_tank_data.csv'\n",
    "ds = TSDataset(file_path=three_tank_filepath, input_len=90, target_len=30, n_samples=100, mean_vals=None, sd_vals=None, seed=42)\n",
    "ds.set_scaler_params()\n",
    "ds.scale_data()\n",
    "three_tank_args = dict(\n",
    "    file_path=three_tank_filepath,\n",
    "    mean_vals=ds.mean_vals,\n",
    "    sd_vals=ds.sd_vals,\n",
    "    n_samples=100,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swat_filepath = '../data/processed/SWaT_Dataset_Normal_v1.parquet'\n",
    "ds0 = TSDataset(file_path=swat_filepath, input_len=90, target_len=30, n_samples=100, mean_vals=None, sd_vals=None, seed=42)\n",
    "ds0.set_scaler_params()\n",
    "ds0.scale_data()\n",
    "swat_args = dict(\n",
    "    file_path=swat_filepath,\n",
    "    mean_vals=ds0.mean_vals,\n",
    "    sd_vals=ds0.sd_vals,\n",
    "    n_samples=100,\n",
    "    seed=42\n",
    ")\n",
    "ds0.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat and plot\n",
    "def plot(ds, idx, original_ds=None):\n",
    "    x, y = ds[idx]\n",
    "    x = np.concatenate([x, y], axis=0)\n",
    "    affected_sensors = [ds.df.columns.get_loc(sensor) for sensor in ds.affected_sensors]\n",
    "    # color affected sensors, the rest in gray\n",
    "    sensor_colors = {}\n",
    "    for i in range(ds.n_features):\n",
    "        if i in affected_sensors:\n",
    "            # plt.plot(x[:, i])\n",
    "            line, = plt.plot(x[:, i])\n",
    "            sensor_colors[i] = line.get_color()\n",
    "        else:\n",
    "            plt.plot(x[:, i], color='grey', alpha=0.5)\n",
    "    if original_ds is not None:\n",
    "        x, y = original_ds[idx]\n",
    "        x = np.concatenate([x, y], axis=0)\n",
    "        for i in range(ds.n_features):\n",
    "            if i in affected_sensors:\n",
    "                plt.plot(x[:, i], linestyle='--', color=sensor_colors[i])\n",
    "    title = f\"{ds.__class__.__name__}, affected sensors: {ds.affected_sensors}\"\n",
    "    max_title_len = 60\n",
    "    if len(title) > max_title_len:\n",
    "        print(title)\n",
    "        title = title[:max_title_len] + \"...\"\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_both(perturbed_ds, perturbed_ds0, idx):\n",
    "    plot(perturbed_ds, idx, original_ds=ds)\n",
    "    plot(perturbed_ds0, idx, original_ds=ds0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique features\n",
    "ds0.df[ds0.discrete_features].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test perturbed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffsetDataset(TSDataset):\n",
    "    \"\"\"Add a constant offset to a random feature of the data.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.offset = self.set_params(severity)\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = min(int(self.n_features * prct_affected_sensors), len(self.continuous_features))\n",
    "        affected_sensors = self.rng.choice(self.continuous_features, n_affected_sensors, replace=False)  # continuous features only\n",
    "\n",
    "        min_offset = 0.\n",
    "        max_offset = 2.\n",
    "        offset = min_offset + severity * (max_offset - min_offset)\n",
    "\n",
    "        return affected_sensors, offset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        x0, y0 = ds[index]\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            x[:, sensor_idx] += self.offset\n",
    "            # y[:, sensor_idx] += self.offset\n",
    "        return x, y\n",
    "\n",
    "severity = 0.5\n",
    "pert_ds = OffsetDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.7,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = OffsetDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = OffsetDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.7,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = OffsetDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DyingSignalDataset(TSDataset):\n",
    "    \"\"\"Multiply the a random feature of the data with a constant factor.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = min(target_prct_affected_sensors * 5, 1)  # no disturbance if flat sensor is hit, therefore we increase the percentage here\n",
    "        self.affected_sensors, self.factor = self.set_params(severity)\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = min(int(self.n_features * prct_affected_sensors), len(self.continuous_features))\n",
    "        affected_sensors = self.rng.choice(self.continuous_features, n_affected_sensors, replace=False)  # continuous features only\n",
    "\n",
    "        min_factor = 1.\n",
    "        max_factor = 0.\n",
    "        factor = min_factor + severity * (max_factor - min_factor)\n",
    "\n",
    "        return affected_sensors, factor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            x[:, sensor_idx] *= self.factor\n",
    "            # y[:, sensor_idx] *= self.factor\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = DyingSignalDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.7,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = DyingSignalDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDataset(TSDataset):\n",
    "    \"\"\"Add Gaussian noise to the data.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.sd = self.set_params(severity)\n",
    "        self.noise = self._create_noise()\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = min(int(self.n_features * prct_affected_sensors), len(self.continuous_features))\n",
    "        affected_sensors = self.rng.choice(self.continuous_features, n_affected_sensors, replace=False)  # continuous features only\n",
    "\n",
    "        min_sd = 0.\n",
    "        max_sd = 1\n",
    "        sd = min_sd + severity * (max_sd - min_sd)\n",
    "\n",
    "        return affected_sensors, sd\n",
    "\n",
    "    def _create_noise(self):\n",
    "        full_noise = self.rng.normal(0, self.sd, (self.n_samples, self.input_len, self.n_features))\n",
    "        noise = np.zeros((self.n_samples, self.input_len, self.n_features))\n",
    "        for i in self.affected_sensors:\n",
    "            idx = self.df.columns.get_loc(i)\n",
    "            noise[:, :, idx] = full_noise[:, :, idx]\n",
    "        return noise.astype(np.float32)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        x_with_noise = x + self.noise[index]\n",
    "        return x_with_noise, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = NoisyDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.7,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = NoisyDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatSensorDataset(TSDataset):\n",
    "    \"\"\"Set a random sensor to the last value for a random duration.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.flat_duration = self.set_params(severity)\n",
    "        self.flat_start_pos = self.rng.integers(1, self.input_len - self.flat_duration + 2, size=(self.n_samples, self.n_features))  # only affected sensors sample from this\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = int(self.n_features * prct_affected_sensors)\n",
    "        affected_sensors = self.rng.choice(self.feature_names, n_affected_sensors, replace=False)\n",
    "\n",
    "        min_flat_duration = 1\n",
    "        max_flat_duration = self.input_len\n",
    "        flat_duration = int(min_flat_duration + severity * (max_flat_duration - min_flat_duration))\n",
    "\n",
    "        return affected_sensors, flat_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            start_pos = self.flat_start_pos[index, sensor_idx]\n",
    "            end_pos = start_pos + self.flat_duration\n",
    "            last_valid_value = x[start_pos - 1, sensor_idx]\n",
    "            x[start_pos:end_pos, sensor_idx] = last_valid_value\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = FlatSensorDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.7,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = FlatSensorDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValueDataset(TSDataset):\n",
    "    \"\"\"Remove a random time window from the data.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=1., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.affected_sensors, self.missing_duration = self.set_params(severity)\n",
    "        self.missing_start_pos = self.rng.integers(0, self.input_len - self.missing_duration, size=(self.n_samples))\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        affected_sensors = self.feature_names.values  # all sensors are affected, regardless of choice of targetr_prct_affected_sensors\n",
    "\n",
    "        min_missing_duration = 1\n",
    "        max_missing_duration = int(self.input_len * 0.5)\n",
    "        missing_duration = min_missing_duration + int(severity * (max_missing_duration - min_missing_duration))\n",
    "\n",
    "        return affected_sensors, missing_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_idx = self.sample_idxs[index]\n",
    "        end_idx = start_idx + self.input_len + self.target_len + self.missing_duration\n",
    "        df_ = self.df.iloc[start_idx:end_idx]\n",
    "\n",
    "        missing_start = self.missing_start_pos[index]\n",
    "        missing_end = missing_start + self.missing_duration\n",
    "        df_ = df_.drop(df_.index[missing_start:missing_end])\n",
    "\n",
    "        x = df_.iloc[:self.input_len].to_numpy().astype(np.float32)\n",
    "        y = df_.iloc[self.input_len:].to_numpy().astype(np.float32)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = MissingValueDataset(\n",
    "    severity=severity,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = MissingValueDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDataset(TSDataset):\n",
    "    \"\"\"Add an outlier to a random sensor of the data.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.hickup_value = self.set_params(severity)\n",
    "        self.fault_mask = self._create_fault_mask()\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = min(int(self.n_features * prct_affected_sensors), len(self.continuous_features))\n",
    "        affected_sensors = self.rng.choice(self.continuous_features, n_affected_sensors, replace=False)  # continuous features only\n",
    "\n",
    "        min_hickup_value = 1\n",
    "        max_hickup_value = 100\n",
    "        hickup_value = min_hickup_value + severity * (max_hickup_value - min_hickup_value)\n",
    "\n",
    "        return affected_sensors, hickup_value\n",
    "\n",
    "    def _create_fault_mask(self):\n",
    "        fault_mask = np.zeros((self.n_samples, self.input_len, self.n_features), dtype=np.float32)\n",
    "        for sample in range(self.n_samples):\n",
    "            for sensor in self.affected_sensors:\n",
    "                sensor_idx = self.df.columns.get_loc(sensor)\n",
    "                hickup_postion = self.rng.integers(1, self.input_len)\n",
    "                fault_mask[sample, hickup_postion, sensor_idx] = self.hickup_value\n",
    "        return fault_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        x_with_fault = x + self.fault_mask[index]\n",
    "        return x_with_fault, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = OutlierDataset(\n",
    "    severity=severity,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = OutlierDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterSamplingDataset(TSDataset):\n",
    "    \"\"\"Irregularly sample the data by warping the time axis of the input sequence.\n",
    "    The time axis is warped by a factor between 1 and 3 during a fixed duration.\n",
    "    After the warped time frame, the sensor remains flat until synchronizing with the original time axis.\n",
    "    \"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.warp_factor, self.warp_duration = self.set_params(severity)\n",
    "        self.warp_start_pos = self.rng.integers(0, int(self.input_len * 0.5), size=(self.n_samples))  # only affected sensors sample from this. \n",
    "        # only first half of the input_len is warped to avoid leakage. same start point for all affected sensors of a sample\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = int(self.n_features * prct_affected_sensors)\n",
    "        affected_sensors = self.rng.choice(self.feature_names, n_affected_sensors, replace=False)\n",
    "\n",
    "        min_warp_factor = 1.\n",
    "        max_warp_factor = 3.\n",
    "        warp_factor = min_warp_factor + severity * (max_warp_factor - min_warp_factor)\n",
    "\n",
    "        warp_duration = int(self.input_len * 0.5)\n",
    "\n",
    "        return affected_sensors, warp_factor, warp_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "\n",
    "        original_time_index = np.arange(self.input_len)\n",
    "        irreg_time = np.full(self.warp_duration, self.warp_factor)\n",
    "        irreg_time_index = np.cumsum(irreg_time) + self.warp_start_pos[index] - 1\n",
    "\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            x[self.warp_start_pos[index]:(self.warp_start_pos[index] + self.warp_duration), sensor_idx] = np.interp(irreg_time_index, original_time_index, x[:, sensor_idx])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = FasterSamplingDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.7,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = FasterSamplingDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.2,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowerSamplingDataset(TSDataset):\n",
    "    \"\"\"Irregularly sample the data by warping the time axis of the input sequence.\n",
    "    The time axis is warped by a factor between 1 and 0 during a fixed duration.\n",
    "    After the warped time frame, the sensor immediately jumps back to the original time axis.\n",
    "    \"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.warp_factor, self.warp_duration = self.set_params(severity)\n",
    "        self.warp_start_pos = self.rng.integers(0, int(self.input_len * 0.5), size=(self.n_samples))  # only affected sensors sample from this. \n",
    "        # only first half of the input_len is warped to avoid leakage. same start point for all affected sensors of a sample\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = int(self.n_features * prct_affected_sensors)\n",
    "        affected_sensors = self.rng.choice(self.feature_names, n_affected_sensors, replace=False)\n",
    "\n",
    "        min_warp_factor = 1.\n",
    "        max_warp_factor = 0.\n",
    "        warp_factor = min_warp_factor + severity * (max_warp_factor - min_warp_factor)\n",
    "\n",
    "        warp_duration = int(self.input_len * 0.5)\n",
    "\n",
    "        return affected_sensors, warp_factor, warp_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "\n",
    "        original_time_index = np.arange(self.input_len)\n",
    "        irreg_time = np.full(self.warp_duration, self.warp_factor)\n",
    "        irreg_time_index = np.cumsum(irreg_time) + self.warp_start_pos[index] - 1\n",
    "\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            x[self.warp_start_pos[index]:(self.warp_start_pos[index] + self.warp_duration), sensor_idx] = np.interp(irreg_time_index, original_time_index, x[:, sensor_idx])\n",
    "        \n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = .5\n",
    "pert_ds = SlowerSamplingDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.7,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = SlowerSamplingDataset(\n",
    "    severity=severity,\n",
    "    target_prct_affected_sensors=0.2,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrongValueDataset(TSDataset):\n",
    "    \"\"\"A discrete sensor or actuator shows a wrong value.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.wrong_duration = self.set_params(severity)\n",
    "        self.wrong_start_pos = self.rng.integers(1, self.input_len - self.wrong_duration + 2, size=(self.n_samples, self.n_features))  # only affected sensors sample from this\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        if len(self.discrete_features) == 0:\n",
    "            raise ValueError(\"No discrete features available.\")\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = min(int(self.n_features * prct_affected_sensors), len(self.discrete_features))\n",
    "        affected_sensors = self.rng.choice(self.discrete_features, n_affected_sensors, replace=False)  # discrete features only\n",
    "\n",
    "        min_wrong_duration = 1\n",
    "        max_wrong_duration = self.input_len\n",
    "        wrong_duration = int(min_wrong_duration + severity * (max_wrong_duration - min_wrong_duration))\n",
    "\n",
    "        return affected_sensors, wrong_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            start_pos = self.wrong_start_pos[index, sensor_idx]\n",
    "            end_pos = start_pos + self.wrong_duration\n",
    "            x[start_pos:end_pos, sensor_idx] = 2  # set a fixed wrong value\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds0 = WrongValueDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot(pert_ds0, 10, ds0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OscillationDataset(TSDataset):\n",
    "    \"\"\"A discrete sensor or actuator oscillates between two values.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.osc_duration = self.set_params(severity)\n",
    "        self.osc_start_pos = self.rng.integers(1, self.input_len - self.osc_duration + 1, size=(self.n_samples, self.n_features))  # only affected sensors sample from this\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        if len(self.discrete_features) == 0:\n",
    "            raise ValueError(\"No discrete features available.\")\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = min(int(self.n_features * prct_affected_sensors), len(self.discrete_features))\n",
    "        affected_sensors = self.rng.choice(self.discrete_features, n_affected_sensors, replace=False)  # discrete features only\n",
    "\n",
    "        min_osc_duration = 1\n",
    "        max_osc_duration = self.input_len - 1  # start value is not affected (technically it is, but it is not visible)\n",
    "        osc_duration = int(min_osc_duration + severity * (max_osc_duration - min_osc_duration))\n",
    "\n",
    "        return affected_sensors, osc_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            start_pos = self.osc_start_pos[index, sensor_idx]\n",
    "            end_pos = start_pos + self.osc_duration\n",
    "\n",
    "            last_value = x[start_pos - 1, sensor_idx]\n",
    "            if self.df[sensor].nunique() == 1:\n",
    "                wrong_value = 1  # if there was only one value, it would be standardized to 0, so set this to 1. Why is this sensor in the data to begin with?\n",
    "            else:\n",
    "                unique_values = self.df[sensor].unique().astype(np.float32)\n",
    "                filtered_values = unique_values[unique_values != last_value]  # force a different value\n",
    "                wrong_value = self.rng.choice(filtered_values)\n",
    "            oscillating_values = self.rng.choice([last_value, wrong_value], size=self.osc_duration, replace=True)\n",
    "            x[start_pos:end_pos, sensor_idx] = oscillating_values\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds0 = OscillationDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot(pert_ds0, 10, ds0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The affected actuator only has two standardized values: [-18.366959  ,   0.05444548] (the -18 is very rare)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClippedIrregularSamplingDataset(TSDataset):\n",
    "    \"\"\"Irregularly sample the data by randomly warping the time axis of the input sequence.\n",
    "    Clip the minimum and maximum values of the time axis warping factor to avoid negative time indices.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors= 0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.warp_factor = self.set_params(severity)\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = int(self.n_features * prct_affected_sensors)\n",
    "        affected_sensors = self.rng.choice(self.continuous_features, n_affected_sensors, replace=False)  # continuous features only\n",
    "\n",
    "        min_warp_factor = 0.\n",
    "        max_warp_factor = 2.\n",
    "        warp_factor = min_warp_factor + severity * (max_warp_factor - min_warp_factor)\n",
    "\n",
    "        return affected_sensors, warp_factor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "\n",
    "        original_time_index = np.arange(x.shape[0])\n",
    "\n",
    "        noisy_time = self.rng.normal(loc=1.0, scale=self.warp_factor, size=x.shape[0] - 1)\n",
    "        noisy_time = np.clip(noisy_time, 0., 2.)  # Make sure the timestamps do not overlap\n",
    "        noisy_time = np.insert(noisy_time, 0, 0)  # First time point is always the same\n",
    "        noisy_time_index = np.cumsum(noisy_time)\n",
    "\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            x[:, sensor_idx] = np.interp(noisy_time_index, original_time_index, x[:, sensor_idx])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = ClippedIrregularSamplingDataset(\n",
    "    severity=severity,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = ClippedIrregularSamplingDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortedIrregularSamplingDataset(TSDataset):\n",
    "    \"\"\"Irregularly sample the data by randomly warping the time axis of the input sequence.\n",
    "    Sort the time indices to avoid negative time indices.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.warp_factor = self.set_params(severity)\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = int(self.n_features * prct_affected_sensors)\n",
    "        affected_sensors = self.rng.choice(self.continuous_features, n_affected_sensors, replace=False)  # continuous features only\n",
    "\n",
    "        min_warp_factor = 0.\n",
    "        max_warp_factor = 2.\n",
    "        warp_factor = min_warp_factor + severity * (max_warp_factor - min_warp_factor)\n",
    "\n",
    "        return affected_sensors, warp_factor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "\n",
    "        original_time_index = np.arange(x.shape[0])\n",
    "\n",
    "        noisy_time = self.rng.normal(loc=1.0, scale=self.warp_factor, size=x.shape[0] - 1)\n",
    "        noisy_time = np.insert(noisy_time, 0, 0)  # First time point is always the same\n",
    "        noisy_time_index = np.cumsum(noisy_time)\n",
    "        noisy_time_index = np.sort(noisy_time_index)\n",
    "\n",
    "        for sensor in self.affected_sensors:\n",
    "            sensor_idx = self.df.columns.get_loc(sensor)\n",
    "            x[:, sensor_idx] = np.interp(noisy_time_index, original_time_index, x[:, sensor_idx])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = SortedIrregularSamplingDataset(\n",
    "    severity=severity,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = SortedIrregularSamplingDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpedWindowDataset(TSDataset):\n",
    "    \"\"\"Irregularly sample the data by warping the time index of a window of the input sequence by a factor of 2.\"\"\"\n",
    "    def __init__(self, severity=1., target_prct_affected_sensors=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert 0 <= severity <= 1, \"Severity must be between 0 and 1.\"\n",
    "        self.severity = severity\n",
    "        self.target_prct_affected_sensors = target_prct_affected_sensors\n",
    "        self.affected_sensors, self.warp_duration = self.set_params(severity)\n",
    "        self.warp_start_pos = self.rng.integers(0, self.input_len - self.warp_duration + 1, size=(self.n_samples))\n",
    "\n",
    "    def set_params(self, severity):\n",
    "        min_prct_affected_sensors = 1 / self.n_features + 1e-9  # at least one sensor must be affected\n",
    "        prct_affected_sensors = max(min_prct_affected_sensors, self.target_prct_affected_sensors)\n",
    "        n_affected_sensors = int(self.n_features * prct_affected_sensors)\n",
    "        affected_sensors = self.rng.choice(self.n_features, n_affected_sensors, replace=False)\n",
    "\n",
    "        min_warp_duration = 1\n",
    "        max_warp_duration = self.input_len\n",
    "        warp_duration = min_warp_duration + int(severity * (max_warp_duration - min_warp_duration))\n",
    "\n",
    "        return affected_sensors, warp_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_idx = self.sample_idxs[index]\n",
    "        warp_start_idx = start_idx + self.warp_start_pos[index]\n",
    "        warp_end_idx = warp_start_idx + 2 * self.warp_duration\n",
    "        end_idx = start_idx + self.input_len + self.target_len + self.warp_duration\n",
    "\n",
    "        pre_warp = self.df.iloc[start_idx:warp_start_idx].to_numpy()\n",
    "        warped = self.df.iloc[warp_start_idx:warp_end_idx:2].to_numpy()\n",
    "        post_warp = self.df.iloc[warp_end_idx:end_idx].to_numpy()\n",
    "\n",
    "        stack = np.vstack([pre_warp, warped, post_warp])\n",
    "\n",
    "        x = stack[:self.input_len].astype(np.float32)\n",
    "        y = stack[self.input_len:].astype(np.float32)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.5\n",
    "pert_ds = WarpedWindowDataset(\n",
    "    severity=severity,\n",
    "    **three_tank_args\n",
    ")\n",
    "pert_ds0 = WarpedWindowDataset(\n",
    "    severity=severity,\n",
    "    **swat_args\n",
    ")\n",
    "plot_both(pert_ds, pert_ds0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warp the frequency domain\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "class FrequencyWarpedDataset(TSDataset):\n",
    "    \"\"\"Warp the frequency domain of the data.\n",
    "    The frequency domain is warped by a random factor that is sampled from a normal distribution.\n",
    "    Args:\n",
    "        warp_factor (float): standard deviation of the normal distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, warp_factor=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.warp_factor = warp_factor\n",
    "        self.df = self.frequency_warp()\n",
    "\n",
    "    def frequency_warp(self):\n",
    "        # Save the original time index\n",
    "        original_time_index = pd.to_numeric(self.df.index) if isinstance(self.df.index, pd.DatetimeIndex) else self.df.index.copy()\n",
    "\n",
    "        # Generate the warped time index\n",
    "        warp = self.rng.normal(loc=1.0, scale=self.warp_factor, size=len(original_time_index) - 1)\n",
    "        warp = np.insert(warp, 0, 1)\n",
    "        noisy_time_index = np.cumsum(warp)\n",
    "        noisy_time_index = np.interp(noisy_time_index, (noisy_time_index[0], noisy_time_index[-1]), (original_time_index[0], original_time_index[-1]))\n",
    "\n",
    "        df_new = pd.DataFrame()\n",
    "        for feature in self.df.columns:\n",
    "            # Interpolate each feature from the original time index to the warped time index\n",
    "            interp_func = interp1d(original_time_index, self.df[feature].values, fill_value='extrapolate')\n",
    "            df_new[feature] = interp_func(noisy_time_index)\n",
    "\n",
    "        # Handling DateTimeIndex for the new DataFrame\n",
    "        if isinstance(self.df.index, pd.DatetimeIndex):\n",
    "            df_new.index = pd.to_datetime(df_new.index)\n",
    "\n",
    "        return df_new\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        # Apply frequency warp to x\n",
    "        x_with_frequency_warp = self.frequency_warp(x)\n",
    "        return x_with_frequency_warp, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft, ifft\n",
    "\n",
    "\n",
    "class FrequencyDataset(TSDataset):\n",
    "    \"\"\"\n",
    "    Transform the time series into the frequency domain, apply noise, and then transform it back.\n",
    "    Args:\n",
    "        noise_sd (float): standard deviation of the Gaussian noise to be added in the frequency domain.\n",
    "        ignore_imaginary (bool): whether to ignore the imaginary part after inverse FFT. Defaults to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_sd=0.01, ignore_imaginary=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.noise_sd = noise_sd\n",
    "        self.ignore_imaginary = ignore_imaginary\n",
    "        self.df = self.apply_frequency_noise()\n",
    "\n",
    "    def apply_frequency_noise(self):\n",
    "        df_new = pd.DataFrame(index=self.df.index)\n",
    "        for feature in self.df.columns:\n",
    "            # FFT transformation\n",
    "            freq_data = fft(self.df[feature].to_numpy())\n",
    "\n",
    "            # Apply Gaussian noise in the frequency domain\n",
    "            noise = self.rng.normal(0, self.noise_sd, len(freq_data))\n",
    "            freq_data_noisy = freq_data + noise\n",
    "\n",
    "            # Inverse FFT to transform back to time domain\n",
    "            time_data_noisy = ifft(freq_data_noisy)\n",
    "\n",
    "            # Check and handle imaginary parts\n",
    "            if not self.ignore_imaginary and np.max(np.abs(time_data_noisy.imag)) > 1e-10:\n",
    "                raise ValueError(\"Significant imaginary component detected after inverse FFT\")\n",
    "\n",
    "            df_new[feature] = time_data_noisy.real\n",
    "\n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FrequencyDataset(\n",
    "    noise_sd=10,\n",
    "    file_path=f\"../data/processed/three_tank_data.csv\",\n",
    "    mean_vals=ds.mean_vals,\n",
    "    sd_vals=ds.sd_vals,\n",
    "    n_samples=100,\n",
    "    seed=42\n",
    ")\n",
    "plot(fds, 1, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not really work for the three tank data. Try SWaT instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the frequency domain\n",
    "def plot_frequency_domain(df, original_df=None):\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    for feature in df.columns:\n",
    "        freq_data = fft(df[feature].to_numpy())\n",
    "        plt.plot(np.abs(freq_data), label=feature)\n",
    "    plt.legend()\n",
    "    if original_df is not None:\n",
    "        for feature in original_df.columns:\n",
    "            freq_data = fft(original_df[feature].to_numpy())\n",
    "            plt.plot(np.abs(freq_data), color='grey', linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_frequency_domain(fds.df, ds.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency_domain(ds.df-fds.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyFilterDataset(TSDataset):\n",
    "    def __init__(self, factor_range=(0.75, 1.25), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if len(factor_range) != 2 or factor_range[0] > factor_range[1]:\n",
    "            raise ValueError(\"factor_range must be a tuple (min_factor, max_factor) with min_factor <= max_factor\")\n",
    "        self.factor_range = factor_range\n",
    "        self.df = self.apply_frequency_perturbation()\n",
    "\n",
    "    def apply_frequency_perturbation(self):\n",
    "        df_new = pd.DataFrame(index=self.df.index)\n",
    "        band = self.rng.choice([0, 1, 2])\n",
    "        factor = self.rng.uniform(*self.factor_range)\n",
    "        for feature in self.df.columns:\n",
    "            feature_data = self.df[feature].to_numpy()\n",
    "            df_new[feature] = self.perturb_feature(feature_data, factor, band)\n",
    "        return df_new\n",
    "\n",
    "    def perturb_feature(self, data, factor, band):\n",
    "        freq_data = fft(data)        \n",
    "        n = len(data)\n",
    "        half_n = (n + 1) // 2  # Correctly handles both even and odd n\n",
    "\n",
    "        # Calculate start and end indices for each band\n",
    "        band_ranges = [(0, half_n//3), (half_n//3, 2*half_n//3), (2*half_n//3, half_n)]\n",
    "        start, end = band_ranges[band]\n",
    "\n",
    "        # Apply the perturbation factor to the selected frequency band\n",
    "        freq_data[start:end] *= factor\n",
    "        freq_data[-end:-start if start != 0 else None] *= factor\n",
    "\n",
    "        time_data_altered = ifft(freq_data).real\n",
    "        return time_data_altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffds = FrequencyFilterDataset(\n",
    "    factor_range=(10, 10),\n",
    "    file_path=f\"../data/processed/three_tank_data.csv\",\n",
    "    mean_vals=ds.mean_vals,\n",
    "    sd_vals=ds.sd_vals,\n",
    "    n_samples=100,\n",
    "    seed=42\n",
    ")\n",
    "plot(ffds, 30, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000\n",
    "ds0 = TSDataset(\n",
    "    file_path=f\"../data/processed/SWaT_Dataset_Normal_v1_sensors.parquet\",\n",
    "    first_column_is_date=True,\n",
    "    n_samples=10,\n",
    "    seed=seed\n",
    ")\n",
    "ds0.set_scaler_params()\n",
    "ds0.scale_data()\n",
    "ffds0 = FrequencyFilterDataset(\n",
    "    factor_range=(0, 0),\n",
    "    file_path=f\"../data/processed/SWaT_Dataset_Normal_v1_sensors.parquet\",\n",
    "    first_column_is_date=True,\n",
    "    n_samples=10,\n",
    "    mean_vals=ds0.mean_vals,\n",
    "    sd_vals=ds0.sd_vals,\n",
    "    seed=seed\n",
    ")\n",
    "plot(ffds0, 1, ds0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_and_frequency_spectrum(time, time_series, sampling_rate):\n",
    "    # FFT transformation\n",
    "    y_fft = np.fft.fft(time_series)\n",
    "    n = len(time_series)\n",
    "\n",
    "    # Frequency bins and amplitude scaling\n",
    "    frequencies = np.fft.fftfreq(n, d=1/sampling_rate)\n",
    "    positive_freqs = frequencies[:n//2]\n",
    "    positive_amplitude = np.abs(y_fft[:n//2]) * 2/n\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    ax[0].plot(time, time_series)\n",
    "    ax[0].set_title('Time Series')\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('Amplitude')\n",
    "\n",
    "    ax[1].stem(positive_freqs, positive_amplitude)\n",
    "    ax[1].set_title('Frequency Spectrum')\n",
    "    ax[1].set_xlabel('Frequency (Hz)')\n",
    "    ax[1].set_ylabel('Amplitude')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_time_series_and_frequency_spectrum(ds0.df.index, ds0.df['LIT101'], 1)\n",
    "plot_time_series_and_frequency_spectrum(ffds0.df.index, ffds0.df['LIT101'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
