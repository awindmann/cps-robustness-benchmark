{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    \"\"\"Time Series Dataset\n",
    "    A sample consists of a (random) time window + consecutive time horizon.\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        file_path (str): path to csv file containing the data\n",
    "        first_column_is_date (bool): whether the first column is a date column. Only used if df is None and csv.\n",
    "        input_len (int): length of input sequence\n",
    "        target_len (int): length of target sequence\n",
    "        stride (int): stride between samples. Only used if n_samples is None and samples are thus drawn sequentially.\n",
    "        n_samples (int): number of samples to draw. If None, all possible samples are drawn sequentially. Else, samples are drawn randomly.\n",
    "        mean_vals (np.array): mean values for each feature. If given, data is rescaled to have zero mean.\n",
    "        sd_vals (np.array): standard deviation values for each feature. If given, data is rescaled to have unit variance.\n",
    "        seed (int): seed for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 df=None,\n",
    "                 file_path=None,\n",
    "                 first_column_is_date=False,\n",
    "                 input_len=100,\n",
    "                 target_len=20,\n",
    "                 stride=1,\n",
    "                 n_samples=None,\n",
    "                 mean_vals=None,\n",
    "                 sd_vals=None,\n",
    "                 seed=42\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "        elif file_path is not None:\n",
    "            if file_path.endswith(\".parquet\"):\n",
    "                self.df = pd.read_parquet(file_path)\n",
    "            elif file_path.endswith(\".csv\"):\n",
    "                self.df = pd.read_csv(file_path)\n",
    "                if first_column_is_date:\n",
    "                    self.df.set_index(pd.to_datetime(self.df.iloc[:,0], format=\"%Y-%m-%d %H:%M:%S\"), inplace=True)\n",
    "                    self.df.drop(self.df.columns[0], axis=1, inplace=True)\n",
    "            else:\n",
    "                raise ValueError(\"File format not supported.\")\n",
    "        else:\n",
    "            raise ValueError(\"Either df or file_path must be given.\")\n",
    "\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        self.stride = stride  # only used if n_samples is None and samples are drawn sequentially\n",
    "\n",
    "        self.random_sampling = n_samples is not None\n",
    "        self.n_samples = self.__len__() if n_samples is None else n_samples\n",
    "        assert self.n_samples <= self.__len__(), \"n_samples must be smaller than the number of possible samples.\"\n",
    "        self.n_features = self.df.shape[1]\n",
    "        self.feature_names = self.df.columns\n",
    "        self.continuous_features, self.discrete_features = self.split_hybrid_data()\n",
    "\n",
    "        # Rescale data if min and max values are given\n",
    "        self.mean_vals = mean_vals\n",
    "        self.sd_vals = sd_vals\n",
    "        if mean_vals is not None and sd_vals is not None:\n",
    "            self.scale_data()\n",
    "\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)  # Using a local random number generator\n",
    "        else:\n",
    "            self.rng = np.random.default_rng()  # Default random generator without a fixed seed\n",
    "\n",
    "        self.sample_idxs = self._create_sample_indices()\n",
    "\n",
    "    def split_hybrid_data(self):\n",
    "        \"\"\"Split the time series data features into continuous and discrete features.\"\"\"\n",
    "        continuous_threshold = 32\n",
    "        continuous_features = [feature for feature in self.df.columns if self.df[feature].nunique() > continuous_threshold]\n",
    "        discrete_features = [feature for feature in self.df.columns if self.df[feature].nunique() <= continuous_threshold]\n",
    "        return continuous_features, discrete_features\n",
    "\n",
    "    def set_scaler_params(self, mean_vals=None, sd_vals=None):\n",
    "        \"\"\"Set the parameters for scaling the data.\n",
    "        Args:\n",
    "            mean_vals (np.array): mean values for each feature. If given, data is rescaled to have zero mean.\n",
    "            sd_vals (np.array): standard deviation values for each feature. If given, data is rescaled to have unit variance.\n",
    "        \"\"\"\n",
    "        self.mean_vals = mean_vals if mean_vals is not None else self.df.mean()\n",
    "        self.sd_vals = sd_vals if sd_vals is not None else self.df.std()\n",
    "\n",
    "    def scale_data(self):\n",
    "        \"\"\"Scale data between min and max values.\"\"\"\n",
    "        assert self.mean_vals is not None and self.sd_vals is not None, \"Mean and standard deviation values must be set first.\"\n",
    "        # Avoid division by zero by replacing sd value of 0 with 1 (for constant features)\n",
    "        self.sd_vals.replace(0, 1.0, inplace=True)\n",
    "        # Standardize the data\n",
    "        self.df = (self.df - self.mean_vals) / self.sd_vals\n",
    "\n",
    "    def inverse_scale_data(self, scaled_data):\n",
    "        df_ = pd.DataFrame(scaled_data, columns=self.df.columns)\n",
    "        return (df_ * self.sd_vals) + self.mean_vals\n",
    "\n",
    "    def _create_sample_indices(self):\n",
    "        \"\"\"Create an array of indices for sampling\"\"\"\n",
    "        if self.random_sampling:\n",
    "            sample_idxs = self.rng.integers(low=0, high=self.df.shape[0] - 2 * self.input_len - self.target_len, size=self.n_samples)  # -2*input_len because some perturbations might require more than input_len time steps\n",
    "        else:\n",
    "            max_n_samples = int((self.df.shape[0] - 2 * self.input_len - self.target_len) / self.stride) + 1  # -2*input_len because some perturbations might require more than input_len time steps\n",
    "            sample_idxs = np.arange(max_n_samples) * self.stride\n",
    "        return sample_idxs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples\"\"\"\n",
    "        if self.random_sampling:\n",
    "            return self.n_samples\n",
    "        else:\n",
    "            return int((self.df.shape[0] - self.input_len - self.target_len) / self.stride) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get one sample.\n",
    "        A sample consists of a time window of length input_len and a consecutive time horizon of length target_len.\n",
    "        Returns:\n",
    "            x (np.array): input sequence\n",
    "            y (np.array): target sequence\n",
    "        \"\"\"\n",
    "        start_idx = self.sample_idxs[index]\n",
    "        end_idx = start_idx + self.input_len + self.target_len\n",
    "        df_ = self.df.iloc[start_idx:end_idx]\n",
    "        x = df_.iloc[:self.input_len].to_numpy().astype(np.float32)\n",
    "        y = df_.iloc[self.input_len:].to_numpy().astype(np.float32)\n",
    "\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TSDataset(file_path='../data/processed/SWaT_Dataset_Normal_v1_sensors.parquet', first_column_is_date=True, input_len=100, target_len=20, stride=1, nb_of_samples=None, mean_vals=None, sd_vals=None, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds[10]\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(ds.random_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = ds.df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0 = TSDataset(df=df0, first_column_is_date=True, input_len=100, target_len=20, stride=1, nb_of_samples=None, mean_vals=None, sd_vals=None, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = ds.df.mean()\n",
    "sd_vals = ds.df.std()\n",
    "ds.set_scaler_params(mean_vals, sd_vals)\n",
    "ds.scale_data()\n",
    "ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.inverse_scale_data(ds.df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.inverse_scale_data(ds.df) - df0).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sample_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = TSDataset(df=df0, first_column_is_date=True, input_len=100, target_len=20, stride=1000, nb_of_samples=None, mean_vals=None, sd_vals=None, seed=42)\n",
    "ds1.sample_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = TSDataset(df=df0, first_column_is_date=True, input_len=100, target_len=20, stride=None, nb_of_samples=10, mean_vals=None, sd_vals=None, seed=42)\n",
    "print(ds2.random_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.sample_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
