{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Dynamic Time Warping a useful measure for the impact of perturbations on a TS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    \"\"\"Time Series Dataset\n",
    "    A sample consists of a (random) time window + consecutive time horizon.\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        file_path (str): path to csv file containing the data\n",
    "        first_column_is_date (bool): whether the first column is a date column. Only used if df is None and csv.\n",
    "        input_len (int): length of input sequence\n",
    "        pred_len (int): length of prediction sequence\n",
    "        stride (int): stride between samples. Only used if nb_of_samples is None and samples are thus drawn sequentially.\n",
    "        nb_of_samples (int): number of samples to draw. If None, all possible samples are drawn sequentially. Else, samples are drawn randomly.\n",
    "        min_vals (np.array): minimum expected values for each feature, used for scaling\n",
    "        max_vals (np.array): maximum expected values for each feature, used for scaling\n",
    "        seed (int): seed for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 df=None,\n",
    "                 file_path=None,\n",
    "                 first_column_is_date=False,\n",
    "                 input_len=100,\n",
    "                 pred_len=20,\n",
    "                 stride=1,\n",
    "                 nb_of_samples=None,\n",
    "                 min_vals=None,\n",
    "                 max_vals=None,\n",
    "                 seed=42\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "        elif file_path is not None:\n",
    "            if file_path.endswith(\".parquet\"):\n",
    "                self.df = pd.read_parquet(file_path)\n",
    "            elif file_path.endswith(\".csv\"):\n",
    "                self.df = pd.read_csv(file_path)\n",
    "                if first_column_is_date:\n",
    "                    self.df.set_index(pd.to_datetime(self.df.iloc[:,0], format=\"%Y-%m-%d %H:%M:%S\"), inplace=True)\n",
    "                    self.df.drop(self.df.columns[0], axis=1, inplace=True)\n",
    "            else:\n",
    "                raise ValueError(\"File format not supported.\")\n",
    "        else:\n",
    "            raise ValueError(\"Either df or file_path must be given.\")\n",
    "\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.stride = stride  # only used if nb_of_samples is None and samples are drawn sequentially\n",
    "\n",
    "        self.random_sampling = nb_of_samples is not None\n",
    "        self.nb_of_samples = self.__len__() if nb_of_samples is None else nb_of_samples\n",
    "        assert self.nb_of_samples <= self.__len__(), \"nb_of_samples must be smaller than the number of possible samples.\"\n",
    "        self.nb_of_features = self.df.shape[1]\n",
    "\n",
    "        # Rescale data if min and max values are given\n",
    "        self.min_vals = min_vals\n",
    "        self.max_vals = max_vals\n",
    "        if min_vals is not None and max_vals is not None:\n",
    "            self.scale_data()\n",
    "\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)  # Using a local random number generator\n",
    "        else:\n",
    "            self.rng = np.random.default_rng()  # Default random generator without a fixed seed\n",
    "\n",
    "        self.sample_idxs = self._create_sample_indices()\n",
    "    \n",
    "    def set_scaler_params(self, min_vals=None, max_vals=None):\n",
    "        \"\"\"Set the parameters for scaling the data.\n",
    "        Args:\n",
    "            min_vals (np.array): minimum values for each feature\n",
    "            max_vals (np.array): maximum values for each feature\n",
    "        \"\"\"\n",
    "        self.min_vals = min_vals if min_vals is not None else self.df.min()\n",
    "        self.max_vals = max_vals if max_vals is not None else self.df.max()\n",
    "\n",
    "    def scale_data(self):\n",
    "        \"\"\"Scale data between min and max values.\"\"\"\n",
    "        assert self.min_vals is not None and self.max_vals is not None, \"min_vals and max_vals must be set via set_scaler_params() first.\"\n",
    "        scale_diff = self.max_vals - self.min_vals\n",
    "        scale_diff.replace(0, 1.0, inplace=True)  # for features that are constant, set scale to 1\n",
    "\n",
    "        self.df = (self.df - self.min_vals) / scale_diff\n",
    "\n",
    "    def inverse_scale_data(self, scaled_data):\n",
    "        df_ = pd.DataFrame(scaled_data, columns=self.df.columns)\n",
    "        return df_ * (self.max_vals - self.min_vals) + self.min_vals\n",
    "\n",
    "    def _create_sample_indices(self):\n",
    "        \"\"\"Create an array of indices for sampling\"\"\"\n",
    "        if self.random_sampling:\n",
    "            sample_idxs = self.rng.integers(low=0, high=self.df.shape[0] - self.input_len - self.pred_len, size=self.nb_of_samples)\n",
    "        else:\n",
    "            max_nb_of_samples = int((self.df.shape[0] - self.input_len - self.pred_len) / self.stride) + 1\n",
    "            sample_idxs = np.arange(max_nb_of_samples) * self.stride\n",
    "        return sample_idxs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples\"\"\"\n",
    "        if self.random_sampling:\n",
    "            return self.nb_of_samples\n",
    "        else:\n",
    "            return int((self.df.shape[0] - self.input_len - self.pred_len) / self.stride) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get one sample.\n",
    "        A sample consists of a time window of length input_len and a consecutive time horizon of length pred_len.\n",
    "        Returns:\n",
    "            x (np.array): input sequence\n",
    "            y (np.array): target sequence\n",
    "        \"\"\"\n",
    "        start_idx = self.sample_idxs[index]\n",
    "        end_idx = start_idx + self.input_len + self.pred_len\n",
    "        df_ = self.df.iloc[start_idx:end_idx]\n",
    "        x = df_.iloc[:self.input_len].values\n",
    "        y = df_.iloc[self.input_len:].values\n",
    "\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "\n",
    "\n",
    "class NoiseDataset(TSDataset):\n",
    "    \"\"\"Add Gaussian noise to the data.\"\"\"\n",
    "    def __init__(self,\n",
    "                 sd=0.01,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.noise = self.rng.normal(0, sd, self.df.shape)\n",
    "        self.df += self.noise\n",
    "\n",
    "\n",
    "class OffsetDataset(TSDataset):\n",
    "    \"\"\"Add a constant offset to a random feature of the data.\"\"\"\n",
    "    def __init__(self,\n",
    "                 offset=0.1,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.offset = offset\n",
    "        feature = self.rng.integers(0, self.nb_of_features)\n",
    "        self.df.iloc[:, feature] += offset\n",
    "\n",
    "\n",
    "class DyingSignalDataset(TSDataset):\n",
    "    \"\"\"Multiply the a random feature of the data with a constant factor.\"\"\"\n",
    "    def __init__(self,\n",
    "                 magnitude=0.9,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.magnitude = magnitude\n",
    "        feature = self.rng.integers(0, self.nb_of_features)\n",
    "        self.df.iloc[:, feature] *= magnitude\n",
    "\n",
    "\n",
    "class TimeWarpedDataset(TSDataset):\n",
    "    \"\"\"Warp the time index of the data.\n",
    "    The time index is warped by a random factor that is sampled from a normal distribution.\n",
    "    Args:\n",
    "        warp_factor (float): standard deviation of the normal distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, warp_factor=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.warp_factor = warp_factor\n",
    "        self.df = self.time_warp()\n",
    "\n",
    "    def time_warp(self):\n",
    "        # Save the original time index\n",
    "        original_time_index = pd.to_numeric(self.df.index) if isinstance(self.df.index, pd.DatetimeIndex) else self.df.index.copy()\n",
    "\n",
    "        # Generate the warped time index\n",
    "        warp = self.rng.normal(loc=1.0, scale=self.warp_factor, size=len(original_time_index) - 1)\n",
    "        warp = np.insert(warp, 0, 1)\n",
    "        warped_time_index = np.cumsum(warp)\n",
    "        warped_time_index = np.interp(warped_time_index, (warped_time_index[0], warped_time_index[-1]), (original_time_index[0], original_time_index[-1]))\n",
    "\n",
    "        df_new = pd.DataFrame()\n",
    "        for feature in self.df.columns:\n",
    "            # Interpolate each feature from the original time index to the warped time index\n",
    "            interp_func = interp1d(original_time_index, self.df[feature].values, fill_value='extrapolate')\n",
    "            df_new[feature] = interp_func(warped_time_index)\n",
    "\n",
    "        # Handling DateTimeIndex for the new DataFrame\n",
    "        if isinstance(self.df.index, pd.DatetimeIndex):\n",
    "            df_new.index = pd.to_datetime(df_new.index)\n",
    "\n",
    "        return df_new\n",
    "\n",
    "\n",
    "class HickUpDataset(TSDataset):\n",
    "    \"\"\"Add a hick-up fault to the data.\n",
    "    The fault is a constant value that is applied to a random feature at a random position.\n",
    "    Args:\n",
    "        min_hick_up (float): minimum value that is added to the feature\n",
    "        max_hick_up (float): maximum value that is added to the feature\n",
    "        fault_probability (float): probability that a fault is applied to a sample\n",
    "    \"\"\"\n",
    "    def __init__(self, min_hick_up=2, max_hick_up=100, fault_probability=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_hick_up = min_hick_up\n",
    "        self.max_hick_up = max_hick_up\n",
    "        self.fault_probability = fault_probability\n",
    "        self.fault_mask = self._create_fault_mask()\n",
    "\n",
    "    def _create_fault_mask(self):\n",
    "        \"\"\"Create a mask that simulates a hick-up fault with a range of anomaly values.\"\"\"\n",
    "        fault_mask = np.zeros((self.nb_of_samples, self.input_len, self.nb_of_features), dtype=np.float32)\n",
    "        for i in range(self.nb_of_samples):\n",
    "            if self.rng.random() < self.fault_probability:\n",
    "                feature = self.rng.integers(0, self.nb_of_features)\n",
    "                pos = self.rng.integers(0, self.input_len)\n",
    "                # Generate a random anomaly value within the specified range\n",
    "                anomaly_value = self.rng.uniform(self.min_hick_up, self.max_hick_up)\n",
    "                fault_mask[i, pos, feature] = anomaly_value\n",
    "        return fault_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        # Apply fault mask to x\n",
    "        x_with_fault = x + self.fault_mask[index]\n",
    "        return x_with_fault, y\n",
    "\n",
    "\n",
    "class DeadSensorDataset(TSDataset):\n",
    "    \"\"\"Simulate a dead sensor.\n",
    "    A dead sensor is simulated by setting the values of a random feature to zero for a random duration.\n",
    "    Args:\n",
    "        min_dead_duration (int): minimum duration of the dead sensor during the input sequence\n",
    "        dead_probability (float): probability that a sensor is dead\n",
    "    \"\"\"\n",
    "    def __init__(self, min_dead_duration=20, dead_probability=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_dead_duration = min_dead_duration\n",
    "        self.dead_probability = dead_probability\n",
    "        self.dead_mask = self._create_dead_mask()\n",
    "\n",
    "    def _create_dead_mask(self):\n",
    "        \"\"\"Create a mask that simulates a dead sensor starting at a random position.\"\"\"\n",
    "        mask = np.ones((self.nb_of_samples, self.input_len + self.pred_len, self.nb_of_features), dtype=np.float32)\n",
    "        for i in range(self.nb_of_samples):\n",
    "            if self.rng.random() < self.dead_probability:\n",
    "                feature = self.rng.integers(0, self.nb_of_features)\n",
    "                start_pos = self.rng.integers(0, self.input_len - self.min_dead_duration -  self.pred_len)\n",
    "                end_pos = self.input_len + self.pred_len\n",
    "                # Apply the dead sensor effect\n",
    "                mask[i, start_pos:end_pos, feature] = 0  # TODO rescaling later leads to a value not equal to zero. Change to 'real' 0 insted?\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        # Apply dead mask to x and y\n",
    "        x_with_dead = x * self.dead_mask[index, :self.input_len, :]\n",
    "        y_with_dead = y * self.dead_mask[index, self.input_len:, :]\n",
    "        return x_with_dead, y_with_dead\n",
    "\n",
    "\n",
    "class FrequencyFilterDataset(TSDataset):\n",
    "    \"\"\"Apply a frequency filter to the data.\n",
    "    The frequency factor is sampled from a uniform distribution and applied to a random frequency band (low, mid, high).\n",
    "    Args:\n",
    "        factor_range (tuple): range of the perturbation factor. The factor is sampled from a uniform distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, factor_range=(0.9, 1.1), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if len(factor_range) != 2 or factor_range[0] > factor_range[1]:\n",
    "            raise ValueError(\"factor_range must be a tuple (min_factor, max_factor) with min_factor <= max_factor\")\n",
    "        self.factor_range = factor_range\n",
    "        self.df = self.apply_frequency_perturbation()\n",
    "\n",
    "    def apply_frequency_perturbation(self):\n",
    "        df_new = pd.DataFrame(index=self.df.index)\n",
    "        band = self.rng.choice([0, 1, 2])\n",
    "        factor = self.rng.uniform(*self.factor_range)\n",
    "        for feature in self.df.columns:\n",
    "            feature_data = self.df[feature].to_numpy()\n",
    "            df_new[feature] = self.perturb_feature(feature_data, factor, band)\n",
    "        return df_new\n",
    "\n",
    "    def perturb_feature(self, data, factor, band):\n",
    "        freq_data = fft(data)        \n",
    "        n = len(data)\n",
    "        half_n = (n + 1) // 2  # Correctly handles both even and odd n\n",
    "\n",
    "        # Calculate start and end indices for each band\n",
    "        band_ranges = [(0, half_n//3), (half_n//3, 2*half_n//3), (2*half_n//3, half_n)]\n",
    "        start, end = band_ranges[band]\n",
    "\n",
    "        # Apply the perturbation factor to the selected frequency band\n",
    "        freq_data[start:end] *= factor\n",
    "        freq_data[-end:-start if start != 0 else None] *= factor\n",
    "\n",
    "        time_data_altered = ifft(freq_data).real\n",
    "        return time_data_altered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class TSDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module for time series data\n",
    "    Args:\n",
    "        file_path (str): path to csv file containing the data\n",
    "        first_column_is_date (bool): whether the first column is a date column\n",
    "        nb_of_train_samples (int): number of train samples to draw. If None, all possible samples are drawn sequentially. Else, samples are drawn randomly.\n",
    "        nb_of_val_and_test_samples (int): number of val/test samples to draw. If None, all possible samples are drawn sequentially. Else, samples are drawn randomly.\n",
    "        train_split (float): fraction of training data\n",
    "        val_split (float): fraction of validation data\n",
    "        purged_fraction (float): fraction of all data to be purged of the start of the validation and test sets. This is done to avoid leakage.\n",
    "        batch_size (int): batch size\n",
    "        num_workers (int): number of workers for dataloader\n",
    "        pin_memory (bool): pin memory for dataloader\n",
    "        persistent_workers (bool): persistent workers for dataloader\n",
    "        seed (int): seed for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 file_path, first_column_is_date=False,\n",
    "                 nb_of_train_samples=None, nb_of_val_and_test_samples=None,\n",
    "                 train_split=0.7, val_split=0.15, purged_fraction=0.01,\n",
    "                 batch_size=64, num_workers=None, pin_memory=True, persistent_workers=False, seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.first_column_is_date = first_column_is_date\n",
    "\n",
    "        self.nb_of_features = None # set in setup()\n",
    "        self.nb_of_train_samples = nb_of_train_samples\n",
    "        self.nb_of_val_and_test_samples = nb_of_val_and_test_samples\n",
    "\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.purged_fraction = purged_fraction\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers if num_workers is not None else os.cpu_count()\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "\n",
    "        # Datasets, populated in setup()\n",
    "        self.ds_train = None\n",
    "        self.ds_val_dict = {}\n",
    "        self.ds_test_dict = {}\n",
    "        \n",
    "        self.dataset_mapping = {\n",
    "            'normal': TSDataset,  # 'normal' is the default scenario, i.e. no perturbation is applied\n",
    "            'noise': NoiseDataset,\n",
    "            'offset': OffsetDataset,\n",
    "            'dying_signal': DyingSignalDataset,\n",
    "            'time_warp': TimeWarpedDataset,\n",
    "            'hick_up': HickUpDataset,\n",
    "            'dead_sensor': DeadSensorDataset,\n",
    "            'frequency_filter': FrequencyFilterDataset\n",
    "        }\n",
    "        self.train_scenario = \"normal\"\n",
    "        self.test_scenarios = self.dataset_mapping.keys()  # scenarios to be used for validation and testing\n",
    "\n",
    "        # min values of training data for scaling, they are set in setup()\n",
    "        self.train_min_vals = None  \n",
    "        self.train_max_vals = None\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "    def _get_split_indices(self, len_df):\n",
    "        train_end_idx = int(len_df * self.train_split)\n",
    "        val_start_idx = train_end_idx\n",
    "        val_end_idx = int(len_df * (self.train_split + self.val_split))\n",
    "\n",
    "        purged_val_start_idx = val_start_idx + int(len_df * self.purged_fraction)\n",
    "        purged_test_start_idx = val_end_idx + int(len_df * self.purged_fraction)\n",
    "\n",
    "        # Ensure indices are within the bounds of the dataframe\n",
    "        assert train_end_idx < len_df\n",
    "        assert purged_val_start_idx < len_df\n",
    "        assert val_end_idx < len_df\n",
    "        assert purged_test_start_idx < len_df\n",
    "\n",
    "        return train_end_idx, purged_val_start_idx, val_end_idx, purged_test_start_idx\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        # Read the complete dataset\n",
    "        if self.file_path.endswith(\".parquet\"):\n",
    "            df_all = pd.read_parquet(self.file_path)\n",
    "        elif self.file_path.endswith(\".csv\"):\n",
    "            df_all = pd.read_csv(self.file_path)\n",
    "            if self.first_column_is_date:\n",
    "                df_all.set_index(pd.to_datetime(df_all.iloc[:,0], format=\"%Y-%m-%d %H:%M:%S\"), inplace=True)\n",
    "                df_all.drop(df_all.columns[0], axis=1, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"File format not supported.\")\n",
    "            \n",
    "        self.nb_of_features = df_all.shape[1]\n",
    "\n",
    "        # Split the dataset into train, val and test\n",
    "        train_end, purged_val_start, val_end, purged_test_start = self._get_split_indices(len(df_all))\n",
    "        df_train = df_all.iloc[:train_end, :]\n",
    "        df_val = df_all.iloc[purged_val_start:val_end, :]\n",
    "        df_test = df_all.iloc[purged_test_start:, :]\n",
    "\n",
    "        # Normalize the datasets using the min and max values of the training set\n",
    "        self.train_min_vals = df_train.min()\n",
    "        self.train_max_vals = df_train.max()\n",
    "\n",
    "        # Create the datasets\n",
    "        self.ds_train = TSDataset(\n",
    "            df=df_train,\n",
    "            nb_of_samples=self.nb_of_train_samples,\n",
    "            min_vals=self.train_min_vals,\n",
    "            max_vals=self.train_max_vals,\n",
    "            seed=self.seed\n",
    "        )\n",
    "        # Create datasets for each test scenario\n",
    "        for scenario in self.test_scenarios:\n",
    "            self.ds_val_dict[scenario] = self.dataset_mapping[scenario](\n",
    "                df=df_val,\n",
    "                nb_of_samples=self.nb_of_val_and_test_samples,\n",
    "                min_vals=self.train_min_vals,\n",
    "                max_vals=self.train_max_vals,\n",
    "                seed=self.seed\n",
    "            )\n",
    "            self.ds_test_dict[scenario] = self.dataset_mapping[scenario](\n",
    "                df=df_test,\n",
    "                nb_of_samples=self.nb_of_val_and_test_samples,\n",
    "                min_vals=self.train_min_vals,\n",
    "                max_vals=self.train_max_vals,\n",
    "                seed=self.seed\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        val_dl_list = [\n",
    "            DataLoader(\n",
    "                self.ds_val_dict[scenario],\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=self.pin_memory,\n",
    "                persistent_workers=self.persistent_workers,\n",
    "                shuffle=False\n",
    "                )\n",
    "            for scenario in self.test_scenarios\n",
    "        ]\n",
    "        return val_dl_list\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        test_dl_list = [\n",
    "            DataLoader(\n",
    "                self.ds_test_dict[scenario],\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=self.pin_memory,\n",
    "                persistent_workers=self.persistent_workers,\n",
    "                shuffle=False\n",
    "                )\n",
    "            for scenario in self.test_scenarios\n",
    "        ]\n",
    "        return test_dl_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TSDataModule(\n",
    "    file_path=\"../data/processed/SWaT_Dataset_Normal_v1_sensors.parquet\",\n",
    "    nb_of_train_samples=1000,\n",
    "    nb_of_val_and_test_samples=100,\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    seed=42\n",
    ")\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dm.train_dataloader()))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.ds_val_dict[\"normal\"].df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.train_max_vals[\"FIT504\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.ds_val_dict[\"normal\"].df[\"FIT504\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def plot_sample(sample, title=None):\n",
    "    x1, x2 = sample\n",
    "    x1 = np.squeeze(x1)\n",
    "    x2 = np.squeeze(x2)\n",
    "\n",
    "    num_features = x1.shape[1]\n",
    "\n",
    "    colors = px.colors.sample_colorscale(\"turbo\", [n/(num_features - 1) for n in range(num_features)])\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i in range(num_features):\n",
    "        color = colors[i]\n",
    "\n",
    "        # Plot x1 (input)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(x1.shape[0]),\n",
    "            y=x1[:, i],\n",
    "            name=f'Feature {i} (Input)',\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color)\n",
    "        ))\n",
    "\n",
    "        # Plot x2 (target)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(x1.shape[0], x1.shape[0] + x2.shape[0]),\n",
    "            y=x2[:, i],\n",
    "            name=f'Feature {i} (Target)',\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color)\n",
    "        ))\n",
    "\n",
    "    fig.add_vline(x=x1.shape[0], line=dict(color=\"black\", width=2, dash=\"dash\"), \n",
    "                  annotation_text=\"Target Start\", annotation_position=\"top right\")  # TODO does not work for fcast overview?\n",
    "    fig.update_xaxes(title_text='Time')\n",
    "    fig.update_layout(width=800, height=500, title=title, \n",
    "                      font_family=\"Serif\", font_size=14,\n",
    "                      margin=dict(l=5, t=50, b=5, r=5))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(dm.ds_val_dict[\"normal\"][0], title=\"Normal data, normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(dm.ds_val_dict[\"noise\"][0], title=\"Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.ds_val_dict[\"noise\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "x_norm = dm.ds_val_dict[\"normal\"][idx][0]\n",
    "\n",
    "scenarios = [\"normal\", \"noise\", \"offset\", \"dying_signal\", \"time_warp\", \"hick_up\", \"dead_sensor\", \"frequency_filter\"]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    x_f = dm.ds_val_dict[scenario][idx][0]\n",
    "    distance, path = fastdtw(x_norm, x_f, dist=euclidean)\n",
    "    print(f\"{scenario}: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dist_df = pd.DataFrame(columns=scenarios)\n",
    "for i in tqdm(range(len(dm.ds_val_dict[\"normal\"]))):\n",
    "    x_norm = dm.ds_val_dict[\"normal\"][i][0]\n",
    "    for scenario in scenarios:\n",
    "        x_f = dm.ds_val_dict[scenario][i][0]\n",
    "        distance, path = fastdtw(x_norm, x_f, dist=euclidean)\n",
    "        dist_df.loc[i, scenario] = distance\n",
    "dist_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format to float, for some reason the dtypes are object\n",
    "dist_df = dist_df.astype(float)\n",
    "dist_df.describe().iloc[[1,2],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
